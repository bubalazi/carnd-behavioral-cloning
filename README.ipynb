{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Cloning - Udacity CarND Project 3\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[initial]: ./docs/initial.png \"Initial distribution prior augmentation\"\n",
    "[track-1-left]: ./docs/tr1_lcr.png \"Track one left\"\n",
    "[track-1-left-flipped]: ./docs/tr1_lcr_flip.png \"Track one left\"\n",
    "[track-1-right]: ./docs/tr1_lcr_right.png \"Track one right\"\n",
    "[track-1-right-flipped]: ./docs/tr1_lcr_right_flip.png \"Track one right\"\n",
    "[combined]: ./docs/combined.png \"Combined all tracks\"\n",
    "[combined-reduced-centre]: ./docs/combined_reduce_centre.png \"Combined all tracks with reduced centre images\"\n",
    "[with-and-without-noise]: ./docs/with_without_noise.png \"With and without noise\"\n",
    "[with-modified-brightness]: ./docs/with_modified_brightness.png \"With modified brightness\"\n",
    "[track-2]: ./docs/tr2_lcr.png \"Track two\"\n",
    "[model]: ./docs/model.png\n",
    "## Summary\n",
    "In this excercise, solely the visual input from three front-facing cameras in a\n",
    "simulator are used\n",
    "to predict the steering angle of the car-simulator. The model was trained on two\n",
    "tracks with varios direction changes and lane changes. Several data-augmentation\n",
    "techniques and CNN architectures where tested and applied in order to improve upon the training process. The resulting model performs well on both tracks, at 17mph\n",
    "on the mountain track and 25mph on the lake track.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "End-to-End (E2E) learning  through convolutional deep neural networks was shown\n",
    "recently to perform better then hand-crafted feature extraction, object detection,\n",
    "behavior inference and decision making in the autonomous vehicle domain [1]. This\n",
    "is attributed to the ability of deep CNNs to innately capture complex details of\n",
    "the scene and their relations. The resulting proposal is that the stream of a monocular\n",
    "vision sensor will be enough to make the car drive itself around complex terrains.\n",
    "\n",
    "### Data Sampling\n",
    "In this excercise a video stream of a car being driven in simulation around two\n",
    "tracks was recorded, a 'lake' track and a 'mountain' track, named track-1 and track-2\n",
    "respectively.\n",
    "\n",
    "\n",
    "\n",
    "***Track-1*** is wide with interrupted lane markings, circumventing a lake.\n",
    "Although being at least twice as wide as the car in the simulator, the track proved\n",
    "to be difficult to be learned due to varios ligthning condition changes, texture\n",
    "changes on the road and surroundings, bridge crossings and interrupted lane markings.\n",
    "\n",
    "Since going in a circle around the track will lead to predominance of turns in\n",
    "the direction of driving, the track was driven in both directions\n",
    "\n",
    "#### Track 1 - Driving Left\n",
    "\n",
    "![][initial]\n",
    "\n",
    "#### Track 1 - Driving Right\n",
    "\n",
    "![][track-1-right]\n",
    "\n",
    "#### Track 2 - Driving in Both lanes\n",
    "\n",
    "***Track-2*** is a two-lane mountain track with continuos markings but with a narrow\n",
    "lane-width. The horizon on this track is in some cases visible, but mostly not. Also\n",
    "the predominant background are dark-green steep hills, steep rocks and abysses.\n",
    "The terrain of track 2 is abbundant of steep climbs, drops and sharp turns.\n",
    "\n",
    "![][track-2]\n",
    "\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "The global lighting condition and scenery color distribution varies greatly\n",
    "between both tracks. To increase the ability of the network to generalize on both\n",
    "tracks, several pre-processing data augmentations where attempted.\n",
    "\n",
    "#### Normalizing the steering angle distributions\n",
    "\n",
    "As can be seen, the steering angle distributions on track-1 depend on the\n",
    "direction being driven and are mostly concentrated around zero - i.e. around straight-line driving . Since it is necessary to train the car to make turns, as good as driving straight in a line, curved sections must be augmented and repeated.\n",
    "\n",
    "\n",
    "##### **Using the Side Cameras**\n",
    "The simulator generates data from three camera locations, centre of car and left/right\n",
    "translated cameras. The left-right translation was assumed to impact the steering angle\n",
    "by a parametric threshold (0.25 < threshold < 0.35). This effectively tripples the\n",
    "dataset by solely adding images that simulate curves.\n",
    "\n",
    "![][track-1-left]\n",
    "\n",
    "##### **Image Flipping**\n",
    "Sections of the data with a greater steering angle then a threshold (0.1 < threshold < 0.4) where flipped and their corresponding steering angles inverted.\n",
    "\n",
    "**Example**: Track-1 Left driving flipped:\n",
    "![][track-1-left-flipped]\n",
    "\n",
    "**Example**: Track-1 Right driving flipped:\n",
    "![][track-1-right-flipped]\n",
    "\n",
    "As can be seen, image distributions now are approximately symmetrical mirror images\n",
    "of each other round the 0 axis.\n",
    "\n",
    "##### **Combining different runs**\n",
    "\n",
    "In order to improve upon the left-right driving differences, runs can be combined.\n",
    "\n",
    "![][combined]\n",
    "\n",
    "##### **Reducing Zero Angle Images**\n",
    "\n",
    "Since zero-angle images are still the most abbundant, their probability of occurrence\n",
    "was reduced to a parametric threshold (0.2 < threshold < 0.4)\n",
    "\n",
    "![][combined-reduced-centre]\n",
    "\n",
    "#### Adding random noise and modifiying image brightness\n",
    "\n",
    "Due to the large difference in backgrounds, methods for global lighting condition\n",
    "and background generalizations were seen as potentially benefitial.\n",
    "\n",
    "##### Random noise addition\n",
    "\n",
    "Images where augmented with random noise, through which each image pixel gets added or\n",
    "substracted a random value picked from a normal distribution with a parametric absolute\n",
    "maximum predefined.\n",
    "\n",
    "![][with-and-without-noise]\n",
    "\n",
    "##### Brightness modification\n",
    "\n",
    "The global pixel values of images got added or subtracted a random value taken from\n",
    "a normal distribution with parametric absolute maximum and truncated to a uint8\n",
    "compatible size.\n",
    "\n",
    "![][with-modified-brightness]\n",
    "\n",
    "## Model\n",
    "\n",
    "Several different models where iterated through. However, the largest difficulty enountered was in regards to the available memory on the GPU that was used for the computations. It was had a memory capacity of 2GB and that showed very fast the incapacity to work on medium to large convolutional nets.  \n",
    "\n",
    "![][model]\n",
    "\n",
    "## Results\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "## References\n",
    "[1] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner,B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller,J. Zhang, et al. ***End to end learning for self-driving cars***, arXiv preprint arXiv:1604.07316, 2016\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
