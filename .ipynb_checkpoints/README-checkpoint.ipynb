{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Cloning - Udacity CarND Project 3\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[initial]: ./docs/initial.png \"Initial distribution prior augmentation\"\n",
    "[track-1-left]: ./docs/tr1_lcr.png \"Track one left\"\n",
    "[track-1-left-flipped]: ./docs/tr1_lcr_flip.png \"Track one left\"\n",
    "[track-1-right]: ./docs/tr1_lcr_right.png \"Track one right\"\n",
    "[track-1-right-flipped]: ./docs/tr1_lcr_right_flip.png \"Track one right\"\n",
    "[combined]: ./docs/combined.png \"Combined all tracks\"\n",
    "[combined-reduced-centre]: ./docs/combined_reduce_centre.png \"Combined all tracks with reduced centre images\"\n",
    "[with-and-without-noise]: ./docs/with_without_noise.png \"With and without noise\"\n",
    "[with-modified-brightness]: ./docs/with_modified_brightness.png \"With modified brightness\"\n",
    "[track-2]: ./docs/tr2_lcr.png \"Track two\"\n",
    "[model]: ./docs/model.png\n",
    "[cropped]: ./docs/cropped.png\n",
    "[output]: ./docs/output.gif\n",
    "[run-precision]: ./docs/model0.png\n",
    "\n",
    "## Summary\n",
    "In this excercise, solely the visual input from three front-facing cameras in a\n",
    "simulator are used\n",
    "to predict the steering angle of the car-simulator. The model was trained on two\n",
    "tracks with varios direction changes and lane changes. Several data-augmentation\n",
    "techniques and CNN architectures where tested and applied in order to improve upon the training process. The resulting model performs well on both tracks, at 17mph\n",
    "on the mountain track and 25mph on the lake track.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "End-to-End (E2E) learning  through convolutional deep neural networks was shown\n",
    "recently to perform better then hand-crafted feature extraction, object detection,\n",
    "behavior inference and decision making in the autonomous vehicle domain [1]. This\n",
    "is attributed to the ability of deep CNNs to innately capture complex details of\n",
    "the scene and their relations. The resulting proposal is that the stream of a monocular\n",
    "vision sensor will be enough to make the car drive itself around complex terrains.\n",
    "\n",
    "### Data Sampling\n",
    "In this excercise a video stream of a car being driven in simulation around two\n",
    "tracks was recorded, a 'lake' track and a 'mountain' track, named track-1 and track-2\n",
    "respectively.\n",
    "\n",
    "\n",
    "\n",
    "***Track-1*** is wide with interrupted lane markings, circumventing a lake.\n",
    "Although being at least twice as wide as the car in the simulator, the track proved\n",
    "to be difficult to be learned due to varios ligthning condition changes, texture\n",
    "changes on the road and surroundings, bridge crossings and interrupted lane markings.\n",
    "\n",
    "Since going in a circle around the track will lead to predominance of turns in\n",
    "the direction of driving, the track was driven in both directions\n",
    "\n",
    "#### Track 1 - Driving Left\n",
    "\n",
    "![][initial]\n",
    "\n",
    "#### Track 1 - Driving Right\n",
    "\n",
    "![][track-1-right]\n",
    "\n",
    "#### Track 2 - Driving in Both lanes\n",
    "\n",
    "***Track-2*** is a two-lane mountain track with continuos markings but with a narrow\n",
    "lane-width. The horizon on this track is in some cases visible, but mostly not. Also\n",
    "the predominant background are dark-green steep hills, steep rocks and abysses.\n",
    "The terrain of track 2 is abbundant of steep climbs, drops and sharp turns.\n",
    "\n",
    "![][track-2]\n",
    "\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "The global lighting condition and scenery color distribution varies greatly\n",
    "between both tracks. To increase the ability of the network to generalize on both\n",
    "tracks, several pre-processing data augmentations where attempted.\n",
    "\n",
    "#### Normalizing the steering angle distributions\n",
    "\n",
    "As can be seen, the steering angle distributions on track-1 depend on the\n",
    "direction being driven and are mostly concentrated around zero - i.e. around straight-line driving . Since it is necessary to train the car to make turns, as good as driving straight in a line, curved sections must be augmented and repeated.\n",
    "\n",
    "\n",
    "##### **Using the Side Cameras**\n",
    "The simulator generates data from three camera locations, centre of car and left/right\n",
    "translated cameras. The left-right translation was assumed to impact the steering angle\n",
    "by a parametric threshold (0.25 < threshold < 0.35). This effectively tripples the\n",
    "dataset by solely adding images that simulate curves.\n",
    "\n",
    "![][track-1-left]\n",
    "\n",
    "##### **Image Flipping**\n",
    "Sections of the data with a greater steering angle then a threshold (0.1 < threshold < 0.4) where flipped and their corresponding steering angles inverted.\n",
    "\n",
    "**Example**: Track-1 Left driving flipped:\n",
    "![][track-1-left-flipped]\n",
    "\n",
    "**Example**: Track-1 Right driving flipped:\n",
    "![][track-1-right-flipped]\n",
    "\n",
    "As can be seen, image distributions now are approximately symmetrical mirror images\n",
    "of each other round the 0 axis.\n",
    "\n",
    "##### **Combining different runs**\n",
    "\n",
    "In order to improve upon the left-right driving differences, runs can be combined.\n",
    "\n",
    "![][combined]\n",
    "\n",
    "##### **Reducing Zero Angle Images**\n",
    "\n",
    "Since zero-angle images are still the most abbundant, their probability of occurrence\n",
    "was reduced to a parametric threshold (0.2 < threshold < 0.4)\n",
    "\n",
    "![][combined-reduced-centre]\n",
    "\n",
    "#### Adding random noise and modifiying image brightness\n",
    "\n",
    "Due to the large difference in backgrounds, methods for global lighting condition\n",
    "and background generalizations were seen as potentially benefitial.\n",
    "\n",
    "##### Random noise addition\n",
    "\n",
    "Images where augmented with random noise, through which each image pixel gets added or\n",
    "substracted a random value picked from a normal distribution with a parametric absolute\n",
    "maximum predefined.\n",
    "\n",
    "![][with-and-without-noise]\n",
    "\n",
    "##### Brightness modification\n",
    "\n",
    "The global pixel values of images got added or subtracted a random value taken from\n",
    "a normal distribution with parametric absolute maximum and truncated to a uint8\n",
    "compatible size.\n",
    "\n",
    "![][with-modified-brightness]\n",
    "\n",
    "## Model\n",
    "\n",
    "Several different models where iterated through. However, the largest difficulty enountered was in regards to the available memory on the GPU that was used for the computations. It had a memory capacity of 2GB and that showed very fast the incapacity to work on medium to large convolutional nets.  \n",
    "\n",
    "### Data Size Reduction\n",
    "\n",
    "#### Using a generator to load batches\n",
    "\n",
    "The first and most crucial strategy to fit the data onto the device was to dynamically load-in data as required, rather then loading all of the data at once. This was achieved using a python generator.\n",
    "\n",
    "#### Cropping redundant data\n",
    "\n",
    "Each image consists of a road segment and background consisting of horizon, sky and terrain features. Since most of the usefull information is located on the bottom part of the image, the top part was cropped. Also, in the image, the complete vehicle is visible with a road segment behind it. This information is also redundant, hence it also can be cropped out. The crop ranges found are 65px from the top and 25px from the bottom.\n",
    "\n",
    "![][cropped]\n",
    "\n",
    "#### Depth vs Width\n",
    "\n",
    "Following ideas defined in [2], building the network was guided by attempting to gradually translate/convolute the spatial information to deep layers, eventually resulting in vector. This meant to minimize information loss in the first layers by lowering strides and increasing filter kernel footprint. It also meant to use average pooling at the start, rather then the more *lossy* max-pooling. Also, much of the design can be attributed to the limited memory size of the GPU device (2GB), on which the network was trained.\n",
    "\n",
    "![][model]\n",
    "Total params: 756,477\n",
    "Trainable params: 756,477\n",
    "Non-trainable params: 0\n",
    "\n",
    "The first convolutional layer, after the image was read-in, normalized and cropped, was chosen to be with a relatively *wide* kernel (5,5) with a stride of (1,1) followed by an average pooling layer, in order to increase information propagation through the network.\n",
    "\n",
    "The next couple of convolutional layers first gradually increase depth and reduce filter size then gradually decrease depth while further reducing filter size. Up until depth decrease, a max-pooling was used. Next no pooling was applied.\n",
    "\n",
    "No dropouts were used in the convolutional layers as this showed to have reduced performance, which can be attributed to spatial-information loss.\n",
    "\n",
    "Next, several fully connected layers were applied, seperated by dropout layers and all gradually decreasing their size to 1.\n",
    "\n",
    "A mean-squared-error loss function and an Adam optimizer at default parameters, incl. learning rate (0.001).\n",
    "\n",
    "### Training\n",
    "\n",
    "The training was performed in two-steps, a pre-training and training step. During pre-training only data from the second track was used for four epochs. During actual training all training sets were combined for four epochs, which achieved best performance.\n",
    "\n",
    "![][run-precision]\n",
    "\n",
    "## Results\n",
    "\n",
    "After a number of attempts and parameter tweaking, a model was build that is able to drive the car accross both tracks for indefinite time.\n",
    "\n",
    "![][output]\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The task was successfully completed.\n",
    "\n",
    "## References\n",
    "[1] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner,B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller,J. Zhang, et al. ***End to end learning for self-driving cars***, arXiv preprint arXiv:1604.07316, 2016\n",
    "\n",
    "[2] Forrest N. Iandola and Song Han and Matthew W. Moskewicz and Khalid Ashraf and William J. Dally and Kurt Keutzer **SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and $<$0.5MB model size**, 2016\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
